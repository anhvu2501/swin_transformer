{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:55:59.401421Z","iopub.execute_input":"2023-01-17T13:55:59.401754Z","iopub.status.idle":"2023-01-17T13:55:59.426472Z","shell.execute_reply.started":"2023-01-17T13:55:59.401679Z","shell.execute_reply":"2023-01-17T13:55:59.425685Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms as T # for simplifying the transforms\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split\nfrom torchvision import models","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:56:17.693821Z","iopub.execute_input":"2023-01-17T13:56:17.694191Z","iopub.status.idle":"2023-01-17T13:56:19.435850Z","shell.execute_reply.started":"2023-01-17T13:56:17.694136Z","shell.execute_reply":"2023-01-17T13:56:19.434839Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"## Now, we import timm, torchvision image models\n!pip install timm # kaggle doesnt have it installed by default\nimport timm\nfrom timm.loss import LabelSmoothingCrossEntropy","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:56:34.975522Z","iopub.execute_input":"2023-01-17T13:56:34.976026Z","iopub.status.idle":"2023-01-17T13:56:47.677194Z","shell.execute_reply.started":"2023-01-17T13:56:34.975992Z","shell.execute_reply":"2023-01-17T13:56:47.676114Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting timm\n  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.1.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (22.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.13.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.28.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.13)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.1.0)\nInstalling collected packages: timm\nSuccessfully installed timm-0.6.12\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:07:24.203260Z","iopub.execute_input":"2023-01-17T14:07:24.204407Z","iopub.status.idle":"2023-01-17T14:07:34.179706Z","shell.execute_reply.started":"2023-01-17T14:07:24.204358Z","shell.execute_reply":"2023-01-17T14:07:34.178550Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m511.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from einops import rearrange","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:07:46.887031Z","iopub.execute_input":"2023-01-17T14:07:46.888090Z","iopub.status.idle":"2023-01-17T14:07:46.900866Z","shell.execute_reply.started":"2023-01-17T14:07:46.888047Z","shell.execute_reply":"2023-01-17T14:07:46.899965Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom tqdm import tqdm\nimport time\nimport copy","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:57:13.305878Z","iopub.execute_input":"2023-01-17T13:57:13.306280Z","iopub.status.idle":"2023-01-17T13:57:13.311793Z","shell.execute_reply.started":"2023-01-17T13:57:13.306244Z","shell.execute_reply":"2023-01-17T13:57:13.310709Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n# remove warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:57:33.845599Z","iopub.execute_input":"2023-01-17T13:57:33.845976Z","iopub.status.idle":"2023-01-17T13:57:33.854283Z","shell.execute_reply.started":"2023-01-17T13:57:33.845943Z","shell.execute_reply":"2023-01-17T13:57:33.853224Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_classes(data_dir):\n    all_data = datasets.ImageFolder(data_dir)\n    return all_data.classes","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:57:47.854923Z","iopub.execute_input":"2023-01-17T13:57:47.855324Z","iopub.status.idle":"2023-01-17T13:57:47.860424Z","shell.execute_reply.started":"2023-01-17T13:57:47.855289Z","shell.execute_reply":"2023-01-17T13:57:47.859057Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_data_loader(data_dir, batch_size, train=False):\n    if train:\n        # train\n        transform = T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomVerticalFlip(),\n            T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n            T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD),\n            T.RandomErasing(p=0.1, value='random')\n        ])\n        train_data = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform=transform)\n        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n        \n        return train_loader, len(train_data)\n    else:\n        # val/test\n        transform = T.Compose([\n            T.Resize(256),\n            T.CenterCrop(224),\n            T.ToTensor(),\n            T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD)\n        ])\n        val_data = datasets.ImageFolder(os.path.join(data_dir, \"valid/\"), transform=transform)\n        test_data = datasets.ImageFolder(os.path.join(data_dir, \"test/\"), transform=transform)\n        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)\n        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n        \n        return val_loader, test_loader, len(val_data), len(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:58:15.810565Z","iopub.execute_input":"2023-01-17T13:58:15.810933Z","iopub.status.idle":"2023-01-17T13:58:15.821874Z","shell.execute_reply.started":"2023-01-17T13:58:15.810902Z","shell.execute_reply":"2023-01-17T13:58:15.820561Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/butterfly-images40-species\"","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:58:41.058043Z","iopub.execute_input":"2023-01-17T13:58:41.058744Z","iopub.status.idle":"2023-01-17T13:58:41.063355Z","shell.execute_reply.started":"2023-01-17T13:58:41.058706Z","shell.execute_reply":"2023-01-17T13:58:41.062362Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"(train_loader, train_data_len) = get_data_loader(dataset_path, 128, train=True)\n(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loader(dataset_path, 32, train=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:59:01.128798Z","iopub.execute_input":"2023-01-17T13:59:01.129185Z","iopub.status.idle":"2023-01-17T13:59:08.222656Z","shell.execute_reply.started":"2023-01-17T13:59:01.129131Z","shell.execute_reply":"2023-01-17T13:59:08.221553Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"classes = get_classes(\"/kaggle/input/butterfly-images40-species/train/\")\nprint(classes, len(classes))","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:59:24.873467Z","iopub.execute_input":"2023-01-17T13:59:24.873863Z","iopub.status.idle":"2023-01-17T13:59:25.004375Z","shell.execute_reply.started":"2023-01-17T13:59:24.873829Z","shell.execute_reply":"2023-01-17T13:59:25.003339Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['ADONIS', 'AFRICAN GIANT SWALLOWTAIL', 'AMERICAN SNOOT', 'AN 88', 'APPOLLO', 'ARCIGERA FLOWER MOTH', 'ATALA', 'ATLAS MOTH', 'BANDED ORANGE HELICONIAN', 'BANDED PEACOCK', 'BANDED TIGER MOTH', 'BECKERS WHITE', 'BIRD CHERRY ERMINE MOTH', 'BLACK HAIRSTREAK', 'BLUE MORPHO', 'BLUE SPOTTED CROW', 'BROOKES BIRDWING', 'BROWN ARGUS', 'BROWN SIPROETA', 'CABBAGE WHITE', 'CAIRNS BIRDWING', 'CHALK HILL BLUE', 'CHECQUERED SKIPPER', 'CHESTNUT', 'CINNABAR MOTH', 'CLEARWING MOTH', 'CLEOPATRA', 'CLODIUS PARNASSIAN', 'CLOUDED SULPHUR', 'COMET MOTH', 'COMMON BANDED AWL', 'COMMON WOOD-NYMPH', 'COPPER TAIL', 'CRECENT', 'CRIMSON PATCH', 'DANAID EGGFLY', 'EASTERN COMA', 'EASTERN DAPPLE WHITE', 'EASTERN PINE ELFIN', 'ELBOWED PIERROT', 'EMPEROR GUM MOTH', 'GARDEN TIGER MOTH', 'GIANT LEOPARD MOTH', 'GLITTERING SAPPHIRE', 'GOLD BANDED', 'GREAT EGGFLY', 'GREAT JAY', 'GREEN CELLED CATTLEHEART', 'GREEN HAIRSTREAK', 'GREY HAIRSTREAK', 'HERCULES MOTH', 'HUMMING BIRD HAWK MOTH', 'INDRA SWALLOW', 'IO MOTH', 'Iphiclus sister', 'JULIA', 'LARGE MARBLE', 'LUNA MOTH', 'MADAGASCAN SUNSET MOTH', 'MALACHITE', 'MANGROVE SKIPPER', 'MESTRA', 'METALMARK', 'MILBERTS TORTOISESHELL', 'MONARCH', 'MOURNING CLOAK', 'OLEANDER HAWK MOTH', 'ORANGE OAKLEAF', 'ORANGE TIP', 'ORCHARD SWALLOW', 'PAINTED LADY', 'PAPER KITE', 'PEACOCK', 'PINE WHITE', 'PIPEVINE SWALLOW', 'POLYPHEMUS MOTH', 'POPINJAY', 'PURPLE HAIRSTREAK', 'PURPLISH COPPER', 'QUESTION MARK', 'RED ADMIRAL', 'RED CRACKER', 'RED POSTMAN', 'RED SPOTTED PURPLE', 'ROSY MAPLE MOTH', 'SCARCE SWALLOW', 'SILVER SPOT SKIPPER', 'SIXSPOT BURNET MOTH', 'SLEEPY ORANGE', 'SOOTYWING', 'SOUTHERN DOGFACE', 'STRAITED QUEEN', 'TROPICAL LEAFWING', 'TWO BARRED FLASHER', 'ULYSES', 'VICEROY', 'WHITE LINED SPHINX MOTH', 'WOOD SATYR', 'YELLOW SWALLOW TAIL', 'ZEBRA LONG WING'] 100\n","output_type":"stream"}]},{"cell_type":"code","source":"dataloaders = {\n    \"train\": train_loader,\n    \"val\": val_loader\n}\ndataset_sizes = {\n    \"train\": train_data_len,\n    \"val\": valid_data_len\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:59:38.463144Z","iopub.execute_input":"2023-01-17T13:59:38.463545Z","iopub.status.idle":"2023-01-17T13:59:38.468805Z","shell.execute_reply.started":"2023-01-17T13:59:38.463511Z","shell.execute_reply":"2023-01-17T13:59:38.467451Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(len(train_loader), len(val_loader), len(test_loader))","metadata":{"execution":{"iopub.status.busy":"2023-01-17T13:59:53.779381Z","iopub.execute_input":"2023-01-17T13:59:53.779742Z","iopub.status.idle":"2023-01-17T13:59:53.785932Z","shell.execute_reply.started":"2023-01-17T13:59:53.779711Z","shell.execute_reply":"2023-01-17T13:59:53.784620Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"99 16 16\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_data_len, valid_data_len, test_data_len)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:00:22.064144Z","iopub.execute_input":"2023-01-17T14:00:22.064522Z","iopub.status.idle":"2023-01-17T14:00:22.070506Z","shell.execute_reply.started":"2023-01-17T14:00:22.064480Z","shell.execute_reply":"2023-01-17T14:00:22.069253Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"12639 500 500\n","output_type":"stream"}]},{"cell_type":"code","source":"# for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:00:07.549390Z","iopub.execute_input":"2023-01-17T14:00:07.549859Z","iopub.status.idle":"2023-01-17T14:00:07.695218Z","shell.execute_reply.started":"2023-01-17T14:00:07.549816Z","shell.execute_reply":"2023-01-17T14:00:07.687053Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"### Model\nclass CyclicShift(nn.Module):\n    def __init__(self, displacement):\n        super().__init__()\n        self.displacement = displacement\n\n    def forward(self, x):\n        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef create_mask(window_size, displacement, upper_lower, left_right):\n    mask = torch.zeros(window_size ** 2, window_size ** 2)\n\n    if upper_lower:\n        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n\n    if left_right:\n        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n        mask[:, -displacement:, :, :-displacement] = float('-inf')\n        mask[:, :-displacement, :, -displacement:] = float('-inf')\n        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n\n    return mask\n\n\ndef get_relative_distances(window_size):\n    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n    distances = indices[None, :, :] - indices[:, None, :]\n    return distances\n\n\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n        super().__init__()\n        inner_dim = head_dim * heads\n\n        self.heads = heads\n        self.scale = head_dim ** -0.5\n        self.window_size = window_size\n        self.relative_pos_embedding = relative_pos_embedding\n        self.shifted = shifted\n\n        if self.shifted:\n            displacement = window_size // 2\n            self.cyclic_shift = CyclicShift(-displacement)\n            self.cyclic_back_shift = CyclicShift(displacement)\n            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n                                                             upper_lower=True, left_right=False), requires_grad=False)\n            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n                                                            upper_lower=False, left_right=True), requires_grad=False)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n\n        if self.relative_pos_embedding:\n            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n        else:\n            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n\n        self.to_out = nn.Linear(inner_dim, dim)\n\n    def forward(self, x):\n        if self.shifted:\n            x = self.cyclic_shift(x)\n\n        b, n_h, n_w, _, h = *x.shape, self.heads\n\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        nw_h = n_h // self.window_size\n        nw_w = n_w // self.window_size\n\n        q, k, v = map(\n            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n\n        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n\n        if self.relative_pos_embedding:\n            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n        else:\n            dots += self.pos_embedding\n\n        if self.shifted:\n            dots[:, :, -nw_w:] += self.upper_lower_mask\n            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n\n        attn = dots.softmax(dim=-1)\n\n        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n        out = self.to_out(out)\n\n        if self.shifted:\n            out = self.cyclic_back_shift(out)\n        return out\n\n\nclass SwinBlock(nn.Module):\n    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n        super().__init__()\n        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n                                                                     heads=heads,\n                                                                     head_dim=head_dim,\n                                                                     shifted=shifted,\n                                                                     window_size=window_size,\n                                                                     relative_pos_embedding=relative_pos_embedding)))\n        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n\n    def forward(self, x):\n        x = self.attention_block(x)\n        x = self.mlp_block(x)\n        return x\n\n\nclass PatchMerging(nn.Module):\n    def __init__(self, in_channels, out_channels, downscaling_factor):\n        super().__init__()\n        self.downscaling_factor = downscaling_factor\n        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n        x = self.linear(x)\n        return x\n\n\nclass StageModule(nn.Module):\n    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n                 relative_pos_embedding):\n        super().__init__()\n        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n\n        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n                                            downscaling_factor=downscaling_factor)\n\n        self.layers = nn.ModuleList([])\n        for _ in range(layers // 2):\n            self.layers.append(nn.ModuleList([\n                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n            ]))\n\n    def forward(self, x):\n        x = self.patch_partition(x)\n        for regular_block, shifted_block in self.layers:\n            x = regular_block(x)\n            x = shifted_block(x)\n        return x.permute(0, 3, 1, 2)\n\n\nclass SwinTransformer(nn.Module):\n    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n        super().__init__()\n\n        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(hidden_dim * 8),\n            nn.Linear(hidden_dim * 8, num_classes)\n        )\n\n    def forward(self, img):\n        x = self.stage1(img)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = x.mean(dim=[2, 3])\n        return self.mlp_head(x)\n\n\ndef swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n\n\ndef swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n\n\ndef swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n\n\ndef swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:14:21.292393Z","iopub.execute_input":"2023-01-17T14:14:21.292737Z","iopub.status.idle":"2023-01-17T14:14:21.335455Z","shell.execute_reply.started":"2023-01-17T14:14:21.292706Z","shell.execute_reply":"2023-01-17T14:14:21.334518Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n\n\ndef swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n\n\ndef swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n\n\ndef swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:05:45.446044Z","iopub.execute_input":"2023-01-17T14:05:45.446759Z","iopub.status.idle":"2023-01-17T14:05:45.454679Z","shell.execute_reply.started":"2023-01-17T14:05:45.446718Z","shell.execute_reply":"2023-01-17T14:05:45.453607Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model = swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24))\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:14:44.989098Z","iopub.execute_input":"2023-01-17T14:14:44.989844Z","iopub.status.idle":"2023-01-17T14:14:45.339678Z","shell.execute_reply.started":"2023-01-17T14:14:44.989806Z","shell.execute_reply":"2023-01-17T14:14:45.338575Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"SwinTransformer(\n  (stage1): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=4, dilation=1, padding=0, stride=4)\n      (linear): Linear(in_features=48, out_features=96, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n                (to_out): Linear(in_features=96, out_features=96, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=96, out_features=384, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=384, out_features=96, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n                (to_out): Linear(in_features=96, out_features=96, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=96, out_features=384, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=384, out_features=96, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (stage2): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n      (linear): Linear(in_features=384, out_features=192, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n                (to_out): Linear(in_features=192, out_features=192, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=192, out_features=768, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=768, out_features=192, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n                (to_out): Linear(in_features=192, out_features=192, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=192, out_features=768, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=768, out_features=192, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (stage3): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n      (linear): Linear(in_features=768, out_features=384, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (1): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (2): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (stage4): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n      (linear): Linear(in_features=1536, out_features=768, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n                (to_out): Linear(in_features=768, out_features=768, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=768, out_features=3072, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=3072, out_features=768, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n                (to_out): Linear(in_features=768, out_features=768, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=768, out_features=3072, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=3072, out_features=768, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (mlp_head): Sequential(\n    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (1): Linear(in_features=768, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n    \nn_inputs = model.mlp_head.\nprint(n_inputs)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:17:19.407989Z","iopub.execute_input":"2023-01-17T14:17:19.408661Z","iopub.status.idle":"2023-01-17T14:17:19.415330Z","shell.execute_reply.started":"2023-01-17T14:17:19.408622Z","shell.execute_reply":"2023-01-17T14:17:19.414341Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (1): Linear(in_features=768, out_features=1000, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for param in model.parameters(): # freeze model\n    param.requires_grad = False\n    \n# n_inputs = model.mlp_head.in_features\nmodel.mlp_head = nn.Sequential(\n    nn.Linear(768, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, len(classes))\n)\nmodel = model.to(device)\nprint(model.mlp_head)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:19:15.517897Z","iopub.execute_input":"2023-01-17T14:19:15.518285Z","iopub.status.idle":"2023-01-17T14:19:18.416468Z","shell.execute_reply.started":"2023-01-17T14:19:15.518253Z","shell.execute_reply":"2023-01-17T14:19:18.415407Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Linear(in_features=768, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.3, inplace=False)\n  (3): Linear(in_features=512, out_features=100, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:19:31.246179Z","iopub.execute_input":"2023-01-17T14:19:31.246546Z","iopub.status.idle":"2023-01-17T14:19:31.253435Z","shell.execute_reply.started":"2023-01-17T14:19:31.246516Z","shell.execute_reply":"2023-01-17T14:19:31.252363Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"SwinTransformer(\n  (stage1): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=4, dilation=1, padding=0, stride=4)\n      (linear): Linear(in_features=48, out_features=96, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n                (to_out): Linear(in_features=96, out_features=96, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=96, out_features=384, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=384, out_features=96, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n                (to_out): Linear(in_features=96, out_features=96, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=96, out_features=384, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=384, out_features=96, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (stage2): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n      (linear): Linear(in_features=384, out_features=192, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n                (to_out): Linear(in_features=192, out_features=192, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=192, out_features=768, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=768, out_features=192, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n                (to_out): Linear(in_features=192, out_features=192, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=192, out_features=768, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=768, out_features=192, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (stage3): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n      (linear): Linear(in_features=768, out_features=384, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (1): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n      (2): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n                (to_out): Linear(in_features=384, out_features=384, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=384, out_features=1536, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=1536, out_features=384, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (stage4): StageModule(\n    (patch_partition): PatchMerging(\n      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n      (linear): Linear(in_features=1536, out_features=768, bias=True)\n    )\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n                (to_out): Linear(in_features=768, out_features=768, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=768, out_features=3072, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=3072, out_features=768, bias=True)\n                )\n              )\n            )\n          )\n        )\n        (1): SwinBlock(\n          (attention_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): WindowAttention(\n                (cyclic_shift): CyclicShift()\n                (cyclic_back_shift): CyclicShift()\n                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n                (to_out): Linear(in_features=768, out_features=768, bias=True)\n              )\n            )\n          )\n          (mlp_block): Residual(\n            (fn): PreNorm(\n              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (fn): FeedForward(\n                (net): Sequential(\n                  (0): Linear(in_features=768, out_features=3072, bias=True)\n                  (1): GELU()\n                  (2): Linear(in_features=3072, out_features=768, bias=True)\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (mlp_head): Sequential(\n    (0): Linear(in_features=768, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.3, inplace=False)\n    (3): Linear(in_features=512, out_features=100, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = LabelSmoothingCrossEntropy()\ncriterion.to(device)\noptimizer = optim.AdamW(model.mlp_head.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:21:42.094522Z","iopub.execute_input":"2023-01-17T14:21:42.094874Z","iopub.status.idle":"2023-01-17T14:21:42.102320Z","shell.execute_reply.started":"2023-01-17T14:21:42.094843Z","shell.execute_reply":"2023-01-17T14:21:42.101423Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# lr scheduler\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:21:59.270423Z","iopub.execute_input":"2023-01-17T14:21:59.270847Z","iopub.status.idle":"2023-01-17T14:21:59.276616Z","shell.execute_reply.started":"2023-01-17T14:21:59.270810Z","shell.execute_reply":"2023-01-17T14:21:59.275529Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n#     loss_values = []\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print(\"-\" * 10)\n        for phase in ['train', 'val']: # do training and validation phase per epoch\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            running_loss = 0.0\n            running_corrects = 0.0\n            \n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation faster\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1) # used for accuracy\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            if phase == 'train':\n                scheduler.step()\n                \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n#             loss_values.append(epoch_loss)\n            \n            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n                \n        print()\n#     my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_values)\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n    \n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:22:14.479540Z","iopub.execute_input":"2023-01-17T14:22:14.479892Z","iopub.status.idle":"2023-01-17T14:22:14.493493Z","shell.execute_reply.started":"2023-01-17T14:22:14.479861Z","shell.execute_reply":"2023-01-17T14:22:14.492391Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T14:34:33.572842Z","iopub.execute_input":"2023-01-17T14:34:33.573265Z","iopub.status.idle":"2023-01-17T14:52:45.625327Z","shell.execute_reply.started":"2023-01-17T14:34:33.573228Z","shell.execute_reply":"2023-01-17T14:52:45.624234Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch 0/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:53<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.4102 Acc: 0.2227\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.2642 Acc: 0.2180\n\nEpoch 1/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:52<00:00,  1.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.3799 Acc: 0.2303\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.2328 Acc: 0.2540\n\nEpoch 2/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:51<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.3502 Acc: 0.2381\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.1985 Acc: 0.2560\n\nEpoch 3/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:51<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.3092 Acc: 0.2524\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.1596 Acc: 0.2740\n\nEpoch 4/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:50<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.2832 Acc: 0.2575\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:03<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.1594 Acc: 0.2740\n\nEpoch 5/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:50<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.2525 Acc: 0.2640\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  5.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.1430 Acc: 0.2860\n\nEpoch 6/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:51<00:00,  1.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.2286 Acc: 0.2688\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.1166 Acc: 0.2960\n\nEpoch 7/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:51<00:00,  1.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.2106 Acc: 0.2719\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.0735 Acc: 0.2920\n\nEpoch 8/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:50<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.1718 Acc: 0.2845\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.0742 Acc: 0.2880\n\nEpoch 9/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:50<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.1520 Acc: 0.2906\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.0331 Acc: 0.3280\n\nEpoch 10/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:52<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.1319 Acc: 0.2912\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.0351 Acc: 0.3000\n\nEpoch 11/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:51<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.1182 Acc: 0.3032\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  5.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.0316 Acc: 0.3120\n\nEpoch 12/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:51<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.1029 Acc: 0.3067\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.0102 Acc: 0.3060\n\nEpoch 13/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:54<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.0876 Acc: 0.3080\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 3.0048 Acc: 0.3400\n\nEpoch 14/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:53<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.0657 Acc: 0.3118\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  5.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 2.9941 Acc: 0.3180\n\nEpoch 15/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:51<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.0594 Acc: 0.3193\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:03<00:00,  4.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 2.9582 Acc: 0.3600\n\nEpoch 16/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:50<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.0298 Acc: 0.3243\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 2.9670 Acc: 0.3380\n\nEpoch 17/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:50<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.0218 Acc: 0.3240\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 2.9469 Acc: 0.3600\n\nEpoch 18/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:50<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 3.0073 Acc: 0.3288\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  5.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"val Loss: 2.9249 Acc: 0.3600\n\nEpoch 19/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 99/99 [00:55<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 2.9957 Acc: 0.3356\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:02<00:00,  6.00it/s]","output_type":"stream"},{"name":"stdout","text":"val Loss: 2.9147 Acc: 0.3660\n\nTraining complete in 18m 12s\nBest Val Acc: 0.3660\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}